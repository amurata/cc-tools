> **[English](../../../plugins/machine-learning-ops/commands/ml-pipeline.md)** | **日本語**

# 機械学習パイプライン - マルチエージェントMLOpsオーケストレーション

次の用途の完全なMLパイプラインを設計・実装：$ARGUMENTS

## 思考

このワークフローは、最新のMLOpsベストプラクティスに従って本番環境対応のMLパイプラインを構築するために、複数の専門エージェントをオーケストレーションします。このアプローチが強調するのは：

- **フェーズベースの調整**：各フェーズは前のアウトプットに基づいて構築され、エージェント間で明確な引き継ぎが行われます
- **最新ツール統合**：実験のためのMLflow/W&B、特徴量のためのFeast/Tecton、サービングのためのKServe/Seldon
- **本番環境ファースト思考**：スケール、監視、信頼性のために設計されたすべてのコンポーネント
- **再現性**：データ、モデル、基盤のバージョン管理
- **継続的改善**：自動化された再学習、A/Bテスト、ドリフト検出

マルチエージェントアプローチにより、各側面がドメインエキスパートによって処理されることを保証：
- データエンジニアがインジェストと品質を処理
- データサイエンティストが特徴量と実験を設計
- MLエンジニアがトレーニングパイプラインを実装
- MLOpsエンジニアが本番環境デプロイメントを処理
- 可観測性エンジニアが監視を保証

## フェーズ1：データと要件分析

<Task>
subagent_type: data-engineer
prompt: |
  次の要件でMLシステムのためのデータパイプラインを分析・設計：$ARGUMENTS

  成果物：
  1. データソース監査とインジェスト戦略：
     - ソースシステムと接続パターン
     - Pydantic/Great Expectationsを使用したスキーマ検証
     - DVCまたはlakeFSによるデータバージョニング
     - インクリメンタルローディングとCDC戦略

  2. データ品質フレームワーク：
     - プロファイリングと統計生成
     - 異常検知ルール
     - データ系譜追跡
     - 品質ゲートとSLA

  3. ストレージアーキテクチャ：
     - Raw/処理済み/特徴量レイヤー
     - パーティショニング戦略
     - 保持ポリシー
     - コスト最適化

  重要なコンポーネントと統合パターンの実装コードを提供してください。
</Task>

<Task>
subagent_type: data-scientist
prompt: |
  次の用途の特徴量エンジニアリングとモデル要件を設計：$ARGUMENTS
  次のデータアーキテクチャを使用：{phase1.data-engineer.output}

  成果物：
  1. 特徴量エンジニアリングパイプライン：
     - 変換仕様
     - 特徴量ストアスキーマ（Feast/Tecton）
     - 統計的検証ルール
     - 欠損データ/外れ値の処理戦略

  2. モデル要件：
     - アルゴリズム選択の根拠
     - パフォーマンス指標とベースライン
     - トレーニングデータ要件
     - 評価基準と閾値

  3. 実験設計：
     - 仮説と成功指標
     - A/Bテスト方法論
     - サンプルサイズ計算
     - バイアス検出アプローチ

  特徴量変換コードと統計的検証ロジックを含めてください。
</Task>

## フェーズ2：モデル開発とトレーニング

<Task>
subagent_type: ml-engineer
prompt: |
  次の要件に基づいてトレーニングパイプラインを実装：{phase1.data-scientist.output}
  次のデータパイプラインを使用：{phase1.data-engineer.output}

  包括的なトレーニングシステムを構築：
  1. トレーニングパイプライン実装：
     - 明確なインターフェースを持つモジュール式トレーニングコード
     - ハイパーパラメータ最適化（Optuna/Ray Tune）
     - 分散学習サポート（Horovod/PyTorch DDP）
     - 交差検証とアンサンブル戦略

  2. 実験追跡セットアップ：
     - MLflow/Weights & Biases統合
     - 指標ロギングと可視化
     - アーティファクト管理（モデル、プロット、データサンプル）
     - 実験比較と分析ツール

  3. モデルレジストリ統合：
     - バージョン管理とタグ付け戦略
     - モデルメタデータと系譜
     - 昇格ワークフロー（dev -> staging -> prod）
     - ロールバック手順

  構成管理を備えた完全なトレーニングコードを提供してください。
</Task>

<Task>
subagent_type: python-pro
prompt: |
  次からのMLコードを最適化し本番環境化：{phase2.ml-engineer.output}

  焦点領域：
  1. コード品質と構造：
     - 本番環境標準にリファクタリング
     - 包括的なエラーハンドリングを追加
     - 構造化形式での適切なロギングを実装
     - 再利用可能なコンポーネントとユーティリティを作成

  2. パフォーマンス最適化：
     - ボトルネックをプロファイルし最適化
     - キャッシング戦略を実装
     - データローディングと前処理を最適化
     - 大規模トレーニングのためのメモリ管理

  3. テストフレームワーク：
     - データ変換のユニットテスト
     - パイプラインコンポーネントの統合テスト
     - モデル品質テスト（不変性、方向性）
     - パフォーマンス回帰テスト

  完全なテストカバレッジを備えた本番環境対応で保守可能なコードを提供してください。
</Task>

## フェーズ3：本番環境デプロイメントとサービング

<Task>
subagent_type: mlops-engineer
prompt: |
  次からのモデルの本番環境デプロイメントを設計：{phase2.ml-engineer.output}
  次からの最適化されたコードを使用：{phase2.python-pro.output}

  実装要件：
  1. モデルサービング基盤：
     - FastAPI/TorchServeによるREST/gRPC API
     - バッチ予測パイプライン（Airflow/Kubeflow）
     - ストリーム処理（Kafka/Kinesis統合）
     - モデルサービングプラットフォーム（KServe/Seldon Core）

  2. デプロイメント戦略：
     - ゼロダウンタイムのためのブルーグリーンデプロイメント
     - トラフィック分割によるカナリアリリース
     - 検証のためのシャドウデプロイメント
     - A/Bテスト基盤

  3. CI/CDパイプライン：
     - GitHub Actions/GitLab CIワークフロー
     - 自動化されたテストゲート
     - デプロイメント前のモデル検証
     - GitOpsデプロイメントのためのArgoCD

  4. Infrastructure as Code：
     - クラウドリソースのためのTerraformモジュール
     - KubernetesデプロイメントのためのHelmチャート
     - 最適化のためのDockerマルチステージビルド
     - Vault/Secrets Managerによるシークレット管理

  完全なデプロイメント構成と自動化スクリプトを提供してください。
</Task>

<Task>
subagent_type: kubernetes-architect
prompt: |
  次からのMLワークロードのためのKubernetes基盤を設計：{phase3.mlops-engineer.output}

  Kubernetes固有の要件：
  1. ワークロードオーケストレーション：
     - Kubeflowによるトレーニングジョブスケジューリング
     - GPUリソース配分と共有
     - スポット/プリエンプティブインスタンス統合
     - 優先度クラスとリソースクォータ

  2. サービング基盤：
     - オートスケーリングのためのHPA/VPA
     - イベント駆動型スケーリングのためのKEDA
     - トラフィック管理のためのIstioサービスメッシュ
     - モデルキャッシングとウォームアップ戦略

  3. ストレージとデータアクセス：
     - トレーニングデータのPVC戦略
     - CSIドライバーによるモデルアーティファクトストレージ
     - 特徴量ストアのための分散ストレージ
     - 推論最適化のためのキャッシュレイヤー

  ML全体プラットフォームのKubernetesマニフェストとHelmチャートを提供してください。
</Task>

## フェーズ4：監視と継続的改善

<Task>
subagent_type: observability-engineer
prompt: |
  次にデプロイされたMLシステムの包括的な監視を実装：{phase3.mlops-engineer.output}
  次のKubernetes基盤を使用：{phase3.kubernetes-architect.output}

  監視フレームワーク：
  1. モデルパフォーマンス監視：
     - 予測精度追跡
     - レイテンシとスループット指標
     - 特徴量重要度シフト
     - ビジネスKPI相関

  2. データとモデルのドリフト検出：
     - 統計的ドリフト検出（KSテスト、PSI）
     - コンセプトドリフト監視
     - 特徴量分布追跡
     - 自動化されたドリフトアラートとレポート

  3. システム可観測性：
     - すべてのコンポーネントのPrometheus指標
     - 可視化のためのGrafanaダッシュボード
     - Jaeger/Zipkinによる分散トレーシング
     - ELK/Lokiによるログ集約

  4. アラートと自動化：
     - PagerDuty/Opsgenie統合
     - 自動化された再学習トリガー
     - パフォーマンス劣化ワークフロー
     - インシデント対応ランブック

  5. コスト追跡：
     - リソース使用率指標
     - モデル/実験ごとのコスト配分
     - 最適化推奨事項
     - 予算アラートとコントロール

  監視構成、ダッシュボード、アラートルールを提供してください。
</Task>

## 構成オプション

- **experiment_tracking**: mlflow | wandb | neptune | clearml
- **feature_store**: feast | tecton | databricks | custom
- **serving_platform**: kserve | seldon | torchserve | triton
- **orchestration**: kubeflow | airflow | prefect | dagster
- **cloud_provider**: aws | azure | gcp | multi-cloud
- **deployment_mode**: realtime | batch | streaming | hybrid
- **monitoring_stack**: prometheus | datadog | newrelic | custom

## 成功基準

1. **データパイプライン成功**：
   - 本番環境でのデータ品質問題 < 0.1%
   - 自動化されたデータ検証の99.9%合格
   - 完全なデータ系譜追跡
   - サブ秒の特徴量サービングレイテンシ

2. **モデルパフォーマンス**：
   - ベースライン指標の達成または超過
   - 再学習前のパフォーマンス劣化 < 5%
   - 統計的有意性を持つ成功したA/Bテスト
   - 24時間以上の未検出モデルドリフトなし

3. **運用の卓越性**：
   - モデルサービングの99.9%アップタイム
   - p99推論レイテンシ < 200ms
   - 5分以内の自動化されたロールバック
   - 1分未満のアラート時間で完全な可観測性

4. **開発速度**：
   - コミットから本番環境まで < 1時間
   - 並列実験実行
   - 再現可能なトレーニング実行
   - セルフサービスモデルデプロイメント

5. **コスト効率**：
   - 基盤の無駄 < 20%
   - 最適化されたリソース配分
   - 負荷に基づく自動スケーリング
   - スポットインスタンス使用率 > 60%

## 最終成果物

完了時、オーケストレーションされたパイプラインは以下を提供：
- 完全な自動化を備えたエンドツーエンドMLパイプライン
- 包括的なドキュメントとランブック
- 本番環境対応のInfrastructure as Code
- 完全な監視とアラートシステム
- 継続的改善のためのCI/CDパイプライン
- コスト最適化とスケーリング戦略
- ディザスタリカバリとロールバック手順
