> **[English](../../../../../plugins/machine-learning-ops/commands/ml-pipeline.md)** | **日本語**

# 機械学習パイプライン - マルチエージェントMLOpsオーケストレーション

以下の要件に対して完全なMLパイプラインを設計および実装: $ARGUMENTS

## 思考

このワークフローは、現代のMLOpsベストプラクティスに従って本番対応のMLパイプラインを構築するために、複数の専門エージェントを調整します。アプローチは以下を強調します:

- **フェーズベースの調整**: 各フェーズは以前の出力に基づいて構築され、エージェント間の明確な引き継ぎがあります
- **現代のツール統合**: 実験用のMLflow/W&B、特徴量用のFeast/Tecton、サービング用のKServe/Seldon
- **本番優先マインドセット**: すべてのコンポーネントが規模、監視、信頼性のために設計されています
- **再現性**: データ、モデル、基盤のバージョン管理
- **継続的改善**: 自動再学習、A/Bテスト、ドリフト検出

マルチエージェントアプローチは、各側面がドメインエキスパートによって処理されることを保証します:
- データエンジニアは取り込みと品質を処理
- データサイエンティストは特徴量と実験を設計
- MLエンジニアは学習パイプラインを実装
- MLOpsエンジニアは本番デプロイメントを処理
- 可観測性エンジニアは監視を保証

## フェーズ1: データと要件分析

<Task>
subagent_type: data-engineer
prompt: |
  以下の要件を持つMLシステムのデータパイプラインを分析および設計: $ARGUMENTS

  成果物:
  1. データソース監査と取り込み戦略:
     - ソースシステムと接続パターン
     - Pydantic/Great Expectationsを使用したスキーマ検証
     - DVCまたはlakeFSによるデータバージョニング
     - 増分ロードとCDC戦略

  2. データ品質フレームワーク:
     - プロファイリングと統計生成
     - 異常検知ルール
     - データ系統追跡
     - 品質ゲートとSLA

  3. ストレージアーキテクチャ:
     - Raw/processed/featureレイヤー
     - パーティショニング戦略
     - 保持ポリシー
     - コスト最適化

  重要なコンポーネントと統合パターンの実装コードを提供してください。
</Task>

<Task>
subagent_type: data-scientist
prompt: |
  以下の要件に対する特徴量エンジニアリングとモデル要件を設計: $ARGUMENTS
  データアーキテクチャを使用: {phase1.data-engineer.output}

  成果物:
  1. 特徴量エンジニアリングパイプライン:
     - 変換仕様
     - 特徴量ストアスキーマ(Feast/Tecton)
     - 統計的検証ルール
     - 欠損データ/外れ値の処理戦略

  2. モデル要件:
     - アルゴリズム選択の根拠
     - パフォーマンスメトリクスとベースライン
     - 学習データ要件
     - 評価基準と閾値

  3. 実験設計:
     - 仮説と成功メトリクス
     - A/Bテスト方法論
     - サンプルサイズ計算
     - バイアス検出アプローチ

  特徴量変換コードと統計的検証ロジックを含めてください。
</Task>

## フェーズ2: モデル開発と学習

<Task>
subagent_type: ml-engineer
prompt: |
  以下の要件に基づいて学習パイプラインを実装: {phase1.data-scientist.output}
  データパイプラインを使用: {phase1.data-engineer.output}

  包括的な学習システムを構築:
  1. 学習パイプライン実装:
     - 明確なインターフェースを持つモジュラー学習コード
     - ハイパーパラメータ最適化(Optuna/Ray Tune)
     - 分散学習サポート(Horovod/PyTorch DDP)
     - 交差検証とアンサンブル戦略

  2. 実験追跡セットアップ:
     - MLflow/Weights & Biases統合
     - メトリクスロギングと可視化
     - アーティファクト管理(モデル、プロット、データサンプル)
     - 実験比較と分析ツール

  3. モデルレジストリ統合:
     - バージョン管理とタグ付け戦略
     - モデルメタデータと系統
     - プロモーションワークフロー(dev -> staging -> prod)
     - ロールバック手順

  設定管理を備えた完全な学習コードを提供してください。
</Task>

<Task>
subagent_type: python-pro
prompt: |
  以下からMLコードを最適化および本番化: {phase2.ml-engineer.output}

  焦点領域:
  1. コード品質と構造:
     - 本番標準にリファクタリング
     - 包括的なエラーハンドリングを追加
     - 構造化フォーマットで適切なロギングを実装
     - 再利用可能なコンポーネントとユーティリティを作成

  2. パフォーマンス最適化:
     - ボトルネックをプロファイルして最適化
     - キャッシング戦略を実装
     - データロードと前処理を最適化
     - 大規模学習のメモリ管理

  3. テストフレームワーク:
     - データ変換のユニットテスト
     - パイプラインコンポーネントの統合テスト
     - モデル品質テスト(不変性、方向性)
     - パフォーマンスリグレッションテスト

  完全なテストカバレッジを備えた本番対応で保守可能なコードを提供してください。
</Task>

## フェーズ3: 本番デプロイメントとサービング

<Task>
subagent_type: mlops-engineer
prompt: |
  以下からのモデルの本番デプロイメントを設計: {phase2.ml-engineer.output}
  最適化されたコードを使用: {phase2.python-pro.output}

  実装要件:
  1. モデルサービング基盤:
     - FastAPI/TorchServeによるREST/gRPC API
     - バッチ予測パイプライン(Airflow/Kubeflow)
     - ストリーム処理(Kafka/Kinesis統合)
     - モデルサービングプラットフォーム(KServe/Seldon Core)

  2. デプロイメント戦略:
     - ゼロダウンタイムのブルーグリーンデプロイメント
     - トラフィック分割によるカナリアリリース
     - 検証のためのシャドウデプロイメント
     - A/Bテスト基盤

  3. CI/CDパイプライン:
     - GitHub Actions/GitLab CIワークフロー
     - 自動化されたテストゲート
     - デプロイメント前のモデル検証
     - GitOpsデプロイメント用のArgoCD

  4. Infrastructure as Code:
     - クラウドリソース用のTerraformモジュール
     - Kubernetesデプロイメント用のHelmチャート
     - 最適化のためのDockerマルチステージビルド
     - Vault/Secrets Managerによるシークレット管理

  完全なデプロイメント設定と自動化スクリプトを提供してください。
</Task>

<Task>
subagent_type: kubernetes-architect
prompt: |
  以下からのMLワークロードのKubernetes基盤を設計: {phase3.mlops-engineer.output}

  Kubernetes固有の要件:
  1. ワークロードオーケストレーション:
     - Kubeflowによる学習ジョブスケジューリング
     - GPUリソース割り当てと共有
     - スポット/プリエンプティブインスタンス統合
     - 優先度クラスとリソースクォータ

  2. サービング基盤:
     - 自動スケーリング用のHPA/VPA
     - イベント駆動スケーリング用のKEDA
     - トラフィック管理用のIstioサービスメッシュ
     - モデルキャッシングとウォームアップ戦略

  3. ストレージとデータアクセス:
     - 学習データ用のPVC戦略
     - CSIドライバーによるモデルアーティファクトストレージ
     - 特徴量ストア用の分散ストレージ
     - 推論最適化用のキャッシュレイヤー

  完全なMLプラットフォーム用のKubernetesマニフェストとHelmチャートを提供してください。
</Task>

## フェーズ4: 監視と継続的改善

<Task>
subagent_type: observability-engineer
prompt: |
  以下にデプロイされたMLシステムの包括的な監視を実装: {phase3.mlops-engineer.output}
  Kubernetes基盤を使用: {phase3.kubernetes-architect.output}

  監視フレームワーク:
  1. モデルパフォーマンス監視:
     - 予測精度追跡
     - レイテンシとスループットメトリクス
     - 特徴量重要度のシフト
     - ビジネスKPI相関

  2. データとモデルのドリフト検出:
     - 統計的ドリフト検出(KSテスト、PSI)
     - 概念ドリフト監視
     - 特徴量分布追跡
     - 自動ドリフトアラートとレポート

  3. システム可観測性:
     - すべてのコンポーネント用のPrometheusメトリクス
     - 可視化用のGrafanaダッシュボード
     - Jaeger/Zipkinによる分散トレーシング
     - ELK/Lokiによるログ集約

  4. アラートと自動化:
     - PagerDuty/Opsgenie統合
     - 自動再学習トリガー
     - パフォーマンス低下ワークフロー
     - インシデント対応ランブック

  5. コスト追跡:
     - リソース使用率メトリクス
     - モデル/実験別のコスト配分
     - 最適化推奨事項
     - 予算アラートと制御

  監視設定、ダッシュボード、アラートルールを提供してください。
</Task>

## 設定オプション

- **experiment_tracking**: mlflow | wandb | neptune | clearml
- **feature_store**: feast | tecton | databricks | custom
- **serving_platform**: kserve | seldon | torchserve | triton
- **orchestration**: kubeflow | airflow | prefect | dagster
- **cloud_provider**: aws | azure | gcp | multi-cloud
- **deployment_mode**: realtime | batch | streaming | hybrid
- **monitoring_stack**: prometheus | datadog | newrelic | custom

## 成功基準

1. **データパイプライン成功**:
   - 本番環境でのデータ品質問題 < 0.1%
   - 自動データ検証の99.9%の成功率
   - 完全なデータ系統追跡
   - サブ秒の特徴量サービングレイテンシ

2. **モデルパフォーマンス**:
   - ベースラインメトリクスの達成または超過
   - 再学習前のパフォーマンス低下 < 5%
   - 統計的有意性を持つ成功したA/Bテスト
   - 24時間以上の未検出モデルドリフトなし

3. **運用優秀性**:
   - モデルサービングの99.9%稼働率
   - p99推論レイテンシ < 200ms
   - 5分以内の自動ロールバック
   - 1分未満のアラート時間での完全な可観測性

4. **開発速度**:
   - コミットから本番まで < 1時間
   - 並列実験実行
   - 再現可能な学習実行
   - セルフサービスモデルデプロイメント

5. **コスト効率**:
   - 基盤の無駄 < 20%
   - 最適化されたリソース割り当て
   - 負荷に基づく自動スケーリング
   - スポットインスタンス使用率 > 60%

## 最終成果物

完了すると、調整されたパイプラインは以下を提供します:
- 完全自動化によるエンドツーエンドMLパイプライン
- 包括的なドキュメンテーションとランブック
- 本番対応のInfrastructure as Code
- 完全な監視とアラートシステム
- 継続的改善のためのCI/CDパイプライン
- コスト最適化とスケーリング戦略
- ディザスタリカバリとロールバック手順
