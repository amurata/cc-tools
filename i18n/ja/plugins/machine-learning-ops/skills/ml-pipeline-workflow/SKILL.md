> **[English](../../../../../../../machine-learning-ops/skills/ml-pipeline-workflow/SKILL.md)** | **日本語**

---
name: ml-pipeline-workflow
description: データ準備からモデル学習、検証、本番デプロイメントまでのエンドツーエンドMLOpsパイプラインを構築します。MLパイプラインの作成、MLOpsプラクティスの実装、またはモデル学習とデプロイメントワークフローの自動化時に使用します。
---

# MLパイプラインワークフロー

データ準備からモデルデプロイメントまでの完全なエンドツーエンドMLOpsパイプラインオーケストレーション。

## 概要

このスキルは、完全なライフサイクルを処理する本番MLパイプラインを構築するための包括的なガイダンスを提供します: データ取り込み → 準備 → 学習 → 検証 → デプロイメント → 監視。

## このスキルを使用する場合

- ゼロから新しいMLパイプラインを構築する
- MLシステムのワークフローオーケストレーションを設計する
- データ → モデル → デプロイメント自動化を実装する
- 再現可能な学習ワークフローをセットアップする
- DAGベースのMLオーケストレーションを作成する
- MLコンポーネントを本番システムに統合する

## このスキルが提供するもの

### コア機能

1. **パイプラインアーキテクチャ**
   - エンドツーエンドワークフロー設計
   - DAGオーケストレーションパターン(Airflow、Dagster、Kubeflow)
   - コンポーネント依存関係とデータフロー
   - エラーハンドリングとリトライ戦略

2. **データ準備**
   - データ検証と品質チェック
   - 特徴量エンジニアリングパイプライン
   - データバージョニングと系統
   - Train/validation/testの分割戦略

3. **モデル学習**
   - 学習ジョブオーケストレーション
   - ハイパーパラメータ管理
   - 実験追跡統合
   - 分散学習パターン

4. **モデル検証**
   - 検証フレームワークとメトリクス
   - A/Bテスト基盤
   - パフォーマンスリグレッション検出
   - モデル比較ワークフロー

5. **デプロイメント自動化**
   - モデルサービングパターン
   - カナリアデプロイメント
   - ブルーグリーンデプロイメント戦略
   - ロールバックメカニズム

### リファレンスドキュメンテーション

詳細なガイドについては`references/`ディレクトリを参照してください:
- **data-preparation.md** - データクリーニング、検証、特徴量エンジニアリング
- **model-training.md** - 学習ワークフローとベストプラクティス
- **model-validation.md** - 検証戦略とメトリクス
- **model-deployment.md** - デプロイメントパターンとサービングアーキテクチャ

### アセットとテンプレート

`assets/`ディレクトリには以下が含まれます:
- **pipeline-dag.yaml.template** - ワークフローオーケストレーション用のDAGテンプレート
- **training-config.yaml** - 学習設定テンプレート
- **validation-checklist.md** - デプロイメント前検証チェックリスト

## 使用パターン

### 基本的なパイプラインセットアップ

```python
# 1. パイプラインステージを定義
stages = [
    "data_ingestion",
    "data_validation",
    "feature_engineering",
    "model_training",
    "model_validation",
    "model_deployment"
]

# 2. 依存関係を設定
# 完全な例はassets/pipeline-dag.yaml.templateを参照
```

### 本番ワークフロー

1. **データ準備フェーズ**
   - ソースから生データを取り込む
   - データ品質チェックを実行
   - 特徴量変換を適用
   - 処理済みデータセットをバージョニング

2. **学習フェーズ**
   - バージョニングされた学習データをロード
   - 学習ジョブを実行
   - 実験とメトリクスを追跡
   - 学習済みモデルを保存

3. **検証フェーズ**
   - 検証テストスイートを実行
   - ベースラインと比較
   - パフォーマンスレポートを生成
   - デプロイメントを承認

4. **デプロイメントフェーズ**
   - モデルアーティファクトをパッケージ化
   - サービング基盤にデプロイ
   - 監視を設定
   - 本番トラフィックを検証

## ベストプラクティス

### パイプライン設計

- **モジュール性**: 各ステージは独立してテスト可能であるべき
- **冪等性**: ステージの再実行は安全であるべき
- **可観測性**: すべてのステージでメトリクスをログ
- **バージョニング**: データ、コード、モデルバージョンを追跡
- **障害ハンドリング**: リトライロジックとアラートを実装

### データ管理

- データ検証ライブラリ(Great Expectations、TFX)を使用
- DVCまたは類似ツールでデータセットをバージョニング
- 特徴量エンジニアリング変換を文書化
- データ系統追跡を維持

### モデル操作

- 学習とサービング基盤を分離
- モデルレジストリ(MLflow、Weights & Biases)を使用
- 新しいモデルの段階的ロールアウトを実装
- モデルパフォーマンスドリフトを監視
- ロールバック機能を維持

### デプロイメント戦略

- シャドウデプロイメントから開始
- 検証にカナリアリリースを使用
- A/Bテスト基盤を実装
- 自動ロールバックトリガーをセットアップ
- レイテンシとスループットを監視

## 統合ポイント

### オーケストレーションツール

- **Apache Airflow**: DAGベースのワークフローオーケストレーション
- **Dagster**: アセットベースのパイプラインオーケストレーション
- **Kubeflow Pipelines**: KubernetesネイティブのMLワークフロー
- **Prefect**: 現代のデータフロー自動化

### 実験追跡

- 実験追跡とモデルレジストリ用のMLflow
- 可視化とコラボレーション用のWeights & Biases
- 学習メトリクス用のTensorBoard

### デプロイメントプラットフォーム

- マネージドML基盤用のAWS SageMaker
- GCPデプロイメント用のGoogle Vertex AI
- Azureクラウド用のAzure ML
- クラウドに依存しないサービング用のKubernetes + KServe

## 段階的開示

基本から始めて徐々に複雑さを追加:

1. **レベル1**: シンプルな線形パイプライン(data → train → deploy)
2. **レベル2**: 検証と監視ステージを追加
3. **レベル3**: ハイパーパラメータチューニングを実装
4. **レベル4**: A/Bテストと段階的ロールアウトを追加
5. **レベル5**: アンサンブル戦略を持つマルチモデルパイプライン

## 一般的なパターン

### バッチ学習パイプライン

```yaml
# assets/pipeline-dag.yaml.templateを参照
stages:
  - name: data_preparation
    dependencies: []
  - name: model_training
    dependencies: [data_preparation]
  - name: model_evaluation
    dependencies: [model_training]
  - name: model_deployment
    dependencies: [model_evaluation]
```

### リアルタイム特徴量パイプライン

```python
# リアルタイム特徴量のストリーム処理
# バッチ学習と組み合わせ
# references/data-preparation.mdを参照
```

### 継続的学習

```python
# スケジュールに基づく自動再学習
# データドリフト検出によってトリガー
# references/model-training.mdを参照
```

## トラブルシューティング

### 一般的な問題

- **パイプライン障害**: 依存関係とデータ可用性を確認
- **学習の不安定性**: ハイパーパラメータとデータ品質を確認
- **デプロイメント問題**: モデルアーティファクトとサービング設定を検証
- **パフォーマンス低下**: データドリフトとモデルメトリクスを監視

### デバッグ手順

1. 各ステージのパイプラインログを確認
2. 境界での入力/出力データを検証
3. コンポーネントを単独でテスト
4. 実験追跡メトリクスを確認
5. モデルアーティファクトとメタデータを検査

## 次のステップ

パイプラインをセットアップした後:

1. 最適化のために**hyperparameter-tuning**スキルを探索
2. MLflow/W&Bのために**experiment-tracking-setup**を学習
3. サービング戦略のために**model-deployment-patterns**を確認
4. 可観測性ツールで監視を実装

## 関連スキル

- **experiment-tracking-setup**: MLflowとWeights & Biases統合
- **hyperparameter-tuning**: 自動化されたハイパーパラメータ最適化
- **model-deployment-patterns**: 高度なデプロイメント戦略
