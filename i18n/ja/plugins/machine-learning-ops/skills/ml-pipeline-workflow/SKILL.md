> **[English](../../../../plugins/machine-learning-ops/skills/ml-pipeline-workflow/SKILL.md)** | **日本語**

---
name: ml-pipeline-workflow
description: データ準備からモデルトレーニング、検証、本番環境デプロイメントまでのエンドツーエンドMLOpsパイプラインを構築。MLパイプラインの作成、MLOpsプラクティスの実装、またはモデルトレーニングとデプロイメントワークフローの自動化時に使用。
---

# MLパイプラインワークフロー

データ準備からモデルデプロイメントまでの完全なエンドツーエンドMLOpsパイプラインオーケストレーション。

## 概要

このスキルは、全ライフサイクルを処理する本番環境MLパイプラインを構築するための包括的なガイダンスを提供：データインジェスト → 準備 → トレーニング → 検証 → デプロイメント → 監視。

## このスキルを使用する場合

- ゼロから新しいMLパイプラインを構築
- MLシステムのワークフローオーケストレーションを設計
- データ → モデル → デプロイメントの自動化を実装
- 再現可能なトレーニングワークフローをセットアップ
- DAGベースのMLオーケストレーションを作成
- MLコンポーネントを本番環境システムに統合

## このスキルが提供するもの

### コア機能

1. **パイプラインアーキテクチャ**
   - エンドツーエンドワークフロー設計
   - DAGオーケストレーションパターン（Airflow、Dagster、Kubeflow）
   - コンポーネント依存関係とデータフロー
   - エラーハンドリングとリトライ戦略

2. **データ準備**
   - データ検証と品質チェック
   - 特徴量エンジニアリングパイプライン
   - データバージョニングと系譜
   - Train/validation/test分割戦略

3. **モデルトレーニング**
   - トレーニングジョブオーケストレーション
   - ハイパーパラメータ管理
   - 実験追跡統合
   - 分散学習パターン

4. **モデル検証**
   - 検証フレームワークと指標
   - A/Bテスト基盤
   - パフォーマンス回帰検出
   - モデル比較ワークフロー

5. **デプロイメント自動化**
   - モデルサービングパターン
   - カナリアデプロイメント
   - ブルーグリーンデプロイメント戦略
   - ロールバックメカニズム

### リファレンスドキュメント

詳細なガイドについては`references/`ディレクトリを参照：
- **data-preparation.md** - データクリーニング、検証、特徴量エンジニアリング
- **model-training.md** - トレーニングワークフローとベストプラクティス
- **model-validation.md** - 検証戦略と指標
- **model-deployment.md** - デプロイメントパターンとサービングアーキテクチャ

### アセットとテンプレート

`assets/`ディレクトリには以下が含まれます：
- **pipeline-dag.yaml.template** - ワークフローオーケストレーション用DAGテンプレート
- **training-config.yaml** - トレーニング構成テンプレート
- **validation-checklist.md** - デプロイメント前検証チェックリスト

## 使用パターン

### 基本パイプラインセットアップ

```python
# 1. パイプラインステージを定義
stages = [
    "data_ingestion",
    "data_validation",
    "feature_engineering",
    "model_training",
    "model_validation",
    "model_deployment"
]

# 2. 依存関係を構成
# 完全な例についてはassets/pipeline-dag.yaml.templateを参照
```

### 本番環境ワークフロー

1. **データ準備フェーズ**
   - ソースから生データをインジェスト
   - データ品質チェックを実行
   - 特徴量変換を適用
   - 処理済みデータセットをバージョン管理

2. **トレーニングフェーズ**
   - バージョン管理されたトレーニングデータをロード
   - トレーニングジョブを実行
   - 実験と指標を追跡
   - トレーニング済みモデルを保存

3. **検証フェーズ**
   - 検証テストスイートを実行
   - ベースラインと比較
   - パフォーマンスレポートを生成
   - デプロイメントを承認

4. **デプロイメントフェーズ**
   - モデルアーティファクトをパッケージ化
   - サービング基盤にデプロイ
   - 監視を構成
   - 本番環境トラフィックを検証

## ベストプラクティス

### パイプライン設計

- **モジュール性**：各ステージは独立してテスト可能であるべき
- **冪等性**：ステージの再実行は安全であるべき
- **可観測性**：すべてのステージで指標をログ
- **バージョニング**：データ、コード、モデルバージョンを追跡
- **障害処理**：リトライロジックとアラートを実装

### データ管理

- データ検証ライブラリを使用（Great Expectations、TFX）
- DVCまたは類似ツールでデータセットをバージョン管理
- 特徴量エンジニアリング変換を文書化
- データ系譜追跡を維持

### モデル運用

- トレーニングとサービング基盤を分離
- モデルレジストリを使用（MLflow、Weights & Biases）
- 新しいモデルの段階的ロールアウトを実装
- モデルパフォーマンスドリフトを監視
- ロールバック機能を維持

### デプロイメント戦略

- シャドウデプロイメントから始める
- 検証のためにカナリアリリースを使用
- A/Bテスト基盤を実装
- 自動化されたロールバックトリガーをセットアップ
- レイテンシとスループットを監視

## 統合ポイント

### オーケストレーションツール

- **Apache Airflow**：DAGベースのワークフローオーケストレーション
- **Dagster**：アセットベースのパイプラインオーケストレーション
- **Kubeflow Pipelines**：KubernetesネイティブなMLワークフロー
- **Prefect**：最新のデータフロー自動化

### 実験追跡

- 実験追跡とモデルレジストリのためのMLflow
- 可視化とコラボレーションのためのWeights & Biases
- トレーニング指標のためのTensorBoard

### デプロイメントプラットフォーム

- 管理されたML基盤のためのAWS SageMaker
- GCPデプロイメントのためのGoogle Vertex AI
- AzureクラウドのためのAzure ML
- クラウドに依存しないサービングのためのKubernetes + KServe

## 段階的開示

基本から始めて徐々に複雑さを追加：

1. **レベル1**：シンプルな線形パイプライン（data → train → deploy）
2. **レベル2**：検証と監視ステージを追加
3. **レベル3**：ハイパーパラメータチューニングを実装
4. **レベル4**：A/Bテストと段階的ロールアウトを追加
5. **レベル5**：アンサンブル戦略を用いたマルチモデルパイプライン

## 一般的なパターン

### バッチトレーニングパイプライン

```yaml
# assets/pipeline-dag.yaml.templateを参照
stages:
  - name: data_preparation
    dependencies: []
  - name: model_training
    dependencies: [data_preparation]
  - name: model_evaluation
    dependencies: [model_training]
  - name: model_deployment
    dependencies: [model_evaluation]
```

### リアルタイム特徴量パイプライン

```python
# リアルタイム特徴量のストリーム処理
# バッチトレーニングと組み合わせ
# references/data-preparation.mdを参照
```

### 継続的トレーニング

```python
# スケジュールに基づく自動化された再学習
# データドリフト検出によってトリガー
# references/model-training.mdを参照
```

## トラブルシューティング

### 一般的な問題

- **パイプライン障害**：依存関係とデータ可用性を確認
- **トレーニング不安定性**：ハイパーパラメータとデータ品質を確認
- **デプロイメント問題**：モデルアーティファクトとサービング構成を検証
- **パフォーマンス劣化**：データドリフトとモデル指標を監視

### デバッグステップ

1. 各ステージのパイプラインログを確認
2. 境界での入出力データを検証
3. コンポーネントを単独でテスト
4. 実験追跡指標を確認
5. モデルアーティファクトとメタデータを検査

## 次のステップ

パイプラインをセットアップした後：

1. 最適化のための**hyperparameter-tuning**スキルを探索
2. MLflow/W&Bのための**experiment-tracking-setup**を学習
3. サービング戦略のための**model-deployment-patterns**を確認
4. 可観測性ツールによる監視を実装

## 関連スキル

- **experiment-tracking-setup**：MLflowとWeights & Biases統合
- **hyperparameter-tuning**：自動化されたハイパーパラメータ最適化
- **model-deployment-patterns**：高度なデプロイメント戦略
